{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#-*-coding:euc-kr-*-\r\n",
    "import os\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'c:\\\\0_HyunSoo\\\\0_Github\\\\yolov5\\\\cutom'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "!dir"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " C ����̺��� �������� �̸��� �����ϴ�.\n",
      " ���� �Ϸ� ��ȣ: 36D9-FAD6\n",
      "\n",
      " c:\\0_HyunSoo\\0_Github\\yolov5\\cutom ���͸�\n",
      "\n",
      "2021-07-28  ���� 12:33    <DIR>          .\n",
      "2021-07-28  ���� 12:33    <DIR>          ..\n",
      "2021-07-28  ���� 01:38         1,114,497 label_data_preprocessing.ipynb\n",
      "2021-07-28  ���� 01:09           191,512 yolov5_aihub.ipynb\n",
      "               2�� ����           1,306,009 ����Ʈ\n",
      "               2�� ���͸�  155,293,892,608 ����Ʈ ����\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "a = \"qwwert.json\"\r\n",
    "b = \"jfe;wklkjbbb.json\"\r\n",
    "print(a[0:-4])\r\n",
    "print(b[0:-4])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "qwwert.\n",
      "jfe;wklkjbbb.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# import glob \r\n",
    "# files = glob.glob('./dataset/labels/*.json')\r\n",
    "# for file in files:\r\n",
    "#     print(file)\r\n",
    "#     with open(file, 'r') as f:\r\n",
    "#         f.read()\r\n",
    "\r\n",
    "import os\r\n",
    "import glob \r\n",
    "import numpy as np\r\n",
    "import json\r\n",
    "\r\n",
    "class_name_to_id_mapping = {\"경찰차\" : 0,\r\n",
    "                            \"구급차\" : 1,\r\n",
    "                            \"기타특장차(견인차, 쓰레기차, 크레인 등)\" : 2,\r\n",
    "                            \"성인(노인포함)\" : 3,\r\n",
    "                            \"어린이\" : 4,\r\n",
    "                            \"자전거\" : 5,\r\n",
    "                            \"오토바이\" : 6,\r\n",
    "                            \"전동휠/전동킥보드/전동휠체어\" : 7,\r\n",
    "                            \"버스(소형,대형)\" : 8, \r\n",
    "                            \"세단\": 9,\r\n",
    "                            \"통학버스(소형,대형)\" : 10,\r\n",
    "                            \"트럭\": 11,\r\n",
    "                            \"SUV/승합차\": 12}\r\n",
    "\r\n",
    "def print_files_in_dir(root_dir, prefix):\r\n",
    "    files = os.listdir(root_dir)\r\n",
    "    k = 0\r\n",
    "    for file in files:\r\n",
    "        print_buffer = []\r\n",
    "        path = os.path.join(root_dir, file)\r\n",
    "        file_name = prefix + path\r\n",
    "#         print(file_name)\r\n",
    "        \r\n",
    "        #JSON 파일읽기\r\n",
    "        with open(path,encoding='UTF8') as json_file:\r\n",
    "            json_data = json.load(json_file)\r\n",
    "        # JSON 내 annotation 배열 설정\r\n",
    "        jsonArray = json_data.get(\"annotations\")\r\n",
    "        # annotation 배열 열고 읽기\r\n",
    "        for list in jsonArray:\r\n",
    "            json_label = list.get(\"label\")\r\n",
    "            json_attribute = list.get(\"attributes\")\r\n",
    "            json_attr_detail = json_attribute.get(json_label)\r\n",
    "#             print(json_attr_detail)\r\n",
    "#             print(json_attr_detail, type(json_attr_detail))\r\n",
    "            try:\r\n",
    "                class_id = class_name_to_id_mapping[json_attr_detail]\r\n",
    "            except KeyError:\r\n",
    "                print(json_attribute,json_attr_detail)\r\n",
    "                print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\r\n",
    "            \r\n",
    "            json_bbox = list.get(\"points\")\r\n",
    "            # bb centerx (x3 + x0) /2 , height (y3 + y0)/2\r\n",
    "            # width x3 - x0, height y3-y0\r\n",
    "            np_bbox = np.array(json_bbox)\r\n",
    "            xmin = np_bbox[0,0] \r\n",
    "            ymin = np_bbox[0,1]\r\n",
    "            xmax = np_bbox[2,0]\r\n",
    "            ymax = np_bbox[2,1]\r\n",
    "#             print(f' xmin: {xmin} /  ymin: {ymin} / xmax: {xmax} / ymax: {ymax}')\r\n",
    "                \r\n",
    "          # Transform the bbox co-ordinates as per the format required by YOLO v5\r\n",
    "            b_center_x = (xmin + xmax)/2 \r\n",
    "            b_center_y = (ymin + ymax)/2\r\n",
    "            b_width    =  xmax - xmin\r\n",
    "            b_height   =  ymax - ymin\r\n",
    "#             print(f' b_center_x: {b_center_x} /  b_center_y: {b_center_y} / b_width: {b_width} / b_height: {b_height}')\r\n",
    "\r\n",
    "            image_w = 1920\r\n",
    "            image_h = 1080     \r\n",
    "\r\n",
    "#             Normalise the co-ordinates by the dimensions of the image\r\n",
    "            b_center_x /= image_w \r\n",
    "            b_center_y /= image_h \r\n",
    "            b_width    /= image_w \r\n",
    "            b_height   /= image_h \r\n",
    "\r\n",
    "#             print(f' b_center_x: {b_center_x} /  b_center_y: {b_center_y} / b_width: {b_width} / b_height: {b_height}')\r\n",
    "\r\n",
    "#             Write the bbox details to the file \r\n",
    "            print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, b_center_x, b_center_y, b_width, b_height))\r\n",
    "    \r\n",
    "#         k += 1\r\n",
    "#         print('------'*10, k)\r\n",
    "        \r\n",
    "    # Name of the file which we have to save \r\n",
    "#         save_file_name = os.path.join(\"root_dir\", info_dict[\"filename\"].replace(\"json\", \"txt\"))\r\n",
    "    \r\n",
    "#     # Save the annotation to disk\r\n",
    "        print(\"\\n\".join(print_buffer), file= open(file_name[0:-5] + \".txt\", \"w\"))\r\n",
    "    \r\n",
    "root_dir = \"C:\\\\0_HyunSoo\\\\datasets\\\\AIhub_labeled_data\\\\원본\\labels_all\"\r\n",
    "print_files_in_dir(root_dir, \"\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{} None\n",
      "Invalid Class. Must be one from  dict_keys(['경찰차', '구급차', '기타특장차(견인차, 쓰레기차, 크레인 등)', '성인(노인포함)', '어린이', '자전거', '오토바이', '전동휠/전동킥보드/전동휠체어', '버스(소형,대형)', '세단', '통학버스(소형,대형)', '트럭', 'SUV/승합차'])\n",
      "{} None\n",
      "Invalid Class. Must be one from  dict_keys(['경찰차', '구급차', '기타특장차(견인차, 쓰레기차, 크레인 등)', '성인(노인포함)', '어린이', '자전거', '오토바이', '전동휠/전동킥보드/전동휠체어', '버스(소형,대형)', '세단', '통학버스(소형,대형)', '트럭', 'SUV/승합차'])\n",
      "{} None\n",
      "Invalid Class. Must be one from  dict_keys(['경찰차', '구급차', '기타특장차(견인차, 쓰레기차, 크레인 등)', '성인(노인포함)', '어린이', '자전거', '오토바이', '전동휠/전동킥보드/전동휠체어', '버스(소형,대형)', '세단', '통학버스(소형,대형)', '트럭', 'SUV/승합차'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image 파일 옮기기 \n",
    "## -  Training Set\n",
    "## -  Validation Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Training Set    \r\n",
    "import shutil\r\n",
    "\r\n",
    "source_file_dir = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/Training/images/'\r\n",
    "destination_dir = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/export/train/'\r\n",
    "file_path = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/train.txt'\r\n",
    "\r\n",
    "f = open(file_path, 'r')\r\n",
    "lines = f.readlines()\r\n",
    "\r\n",
    "for line in lines:\r\n",
    "    line = line.strip()\r\n",
    "    source_file = source_file_dir + line\r\n",
    "    destination_file = destination_dir + line\r\n",
    "    shutil.move(source_file,destination_file)\r\n",
    "f.close()\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Valisate Set    \r\n",
    "import shutil\r\n",
    "\r\n",
    "source_file_dir = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/Training/images/'\r\n",
    "destination_dir = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/export/val/'\r\n",
    "file_path = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/val.txt'\r\n",
    "\r\n",
    "f = open(file_path, 'r')\r\n",
    "lines = f.readlines()\r\n",
    "\r\n",
    "for line in lines:\r\n",
    "    line = line.strip()\r\n",
    "    source_file = source_file_dir+line\r\n",
    "    destination_file = destination_dir +line\r\n",
    "    shutil.move(source_file,destination_file)\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Label 파일옮기기 txt\n",
    "## - Training\n",
    "## - Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#validation Set    \r\n",
    "import shutil\r\n",
    "import os\r\n",
    "\r\n",
    "\r\n",
    "source_file_dir = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/원본/labels_all/'\r\n",
    "destination_dir = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/export/labels/train/'\r\n",
    "\r\n",
    "file_path = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/train.txt'\r\n",
    "f = open(file_path, 'r')\r\n",
    "\r\n",
    "train_lists = f.readlines()\r\n",
    "\r\n",
    "for list_name in train_lists:\r\n",
    "    temp = list_name.strip().strip('\\n').split('\\\\')[-1].split(\".\")[0]\r\n",
    "    source = source_file_dir +temp + '.txt'\r\n",
    "    dest = destination_dir + temp + '.txt'\r\n",
    "    shutil.move(source, dest)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#validation Set    \r\n",
    "import shutil\r\n",
    "import os\r\n",
    "\r\n",
    "\r\n",
    "source_file_dir = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/원본/labels_all/'\r\n",
    "destination_dir = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/export/labels/val/'\r\n",
    "\r\n",
    "file_path = 'C:/0_HyunSoo/datasets/AIhub_labeled_data/val.txt'\r\n",
    "f = open(file_path, 'r')\r\n",
    "\r\n",
    "train_lists = f.readlines()\r\n",
    "\r\n",
    "for list_name in train_lists:\r\n",
    "    temp = list_name.strip().strip('\\n').split('\\\\')[-1].split(\".\")[0]\r\n",
    "    source = source_file_dir +temp + '.txt'\r\n",
    "    dest = destination_dir + temp + '.txt'\r\n",
    "    shutil.move(source, dest)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# 이름바꾸기\r\n",
    "import shutil\r\n",
    "import os\r\n",
    "from glob import glob\r\n",
    "\r\n",
    "source_file_dir = 'C:/0_HyunSoo/0_Github/yolov5/custom_data/yolov5_dataset/labels/train/'\r\n",
    "destination_dir = 'C:/0_HyunSoo/0_Github/yolov5/custom_data/yolov5_dataset/labels/train/'\r\n",
    "\r\n",
    "temp = source_file_dir + \"*.txt\"\r\n",
    "file_lists = glob(temp)\r\n",
    "\r\n",
    "for files in file_lists:\r\n",
    "    temp = files.strip().strip(\"\\n\").split(\"\\\\\")[1]\r\n",
    "    source = source_file_dir +temp\r\n",
    "    dest = destination_dir + temp.split(\"_\")[-1]\r\n",
    "    shutil.move(source, dest)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('yolov5': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "c4609e5c8ff7eb427eee899824a3741b2e70e306c3fda63ba1e46030b76e80de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}